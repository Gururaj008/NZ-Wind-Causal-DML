{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MZ6fXWicDl8"
      },
      "source": [
        "# **Causal Inference for Wind Power: Disentangling Weather Effects in New Zealand's Grid**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAwnYjn8wiZ8",
        "outputId": "522d0d80-965c-4335-a5a7-4e7141a4f3c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating the folder structure**"
      ],
      "metadata": {
        "id": "9FkHtjNmyKuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Global Config\n",
        "BASE_DIR = os.getcwd()\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"Data\", \"Final_dataset\")\n",
        "PLOT_DIR = os.path.join(BASE_DIR, \"Results\", \"Plots\")\n",
        "TABLE_DIR = os.path.join(BASE_DIR, \"Results\", \"Tables\")\n",
        "\n",
        "for p in [PLOT_DIR, TABLE_DIR]:\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "\n",
        "FILES = {\n",
        "    \"North Island\": \"ENHANCED_DATASET_NORTH_ISLAND.csv\",\n",
        "    \"South Island\": \"ENHANCED_DATASET_SOUTH_ISLAND.csv\"\n",
        "}"
      ],
      "metadata": {
        "id": "3zph2u3zyK59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO81wLjvPeej"
      },
      "source": [
        "### **EDA Suite**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-bsX1X3Pjui"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "# Consistent Color Scheme for Distributions\n",
        "ISLAND_COLORS = {\n",
        "    \"North Island\": \"#FF8C00\",  # Dark Orange\n",
        "    \"South Island\": \"#1E90FF\"   # Dodger Blue\n",
        "}\n",
        "\n",
        "def run_eda_suite(name, filename):\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Error: {filename} not found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nProcessing EDA for: {name}\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Filter out curtailment (Zero floor) for cleaner stats\n",
        "    clean_df = df[df['power_mw'] > 0.1].copy()\n",
        "\n",
        "    # Get specific color for this island\n",
        "    color = ISLAND_COLORS[name]\n",
        "\n",
        "    # --- 1. FEATURE RANKING (Spearman Correlation + P-Values) ---\n",
        "    target = 'power_mw'\n",
        "    features = ['wind_speed_100m', 'wind_cubed', 'theoretical_energy',\n",
        "                'air_density_kgm3', 'temp_c', 'pressure_hpa',\n",
        "                'turbulence_proxy', 'ramp_rate']\n",
        "\n",
        "    stats_data = []\n",
        "    for feat in features:\n",
        "        corr, p_val = spearmanr(clean_df[feat], clean_df[target])\n",
        "        stats_data.append({\n",
        "            'Feature': feat,\n",
        "            'Spearman_r': corr,\n",
        "            'P_Value': p_val\n",
        "        })\n",
        "\n",
        "    # Create DataFrame and Sort\n",
        "    stats_df = pd.DataFrame(stats_data).sort_values(by='Spearman_r', ascending=False)\n",
        "\n",
        "    # Save Table\n",
        "    table_path = os.path.join(TABLE_DIR, f\"Feature_Correlations_{name.replace(' ', '_')}.csv\")\n",
        "    stats_df.to_csv(table_path, index=False)\n",
        "    print(f\"Saved Correlation Table: {os.path.basename(table_path)}\")\n",
        "\n",
        "    # Print Interpretation\n",
        "    top_feature = stats_df.iloc[0]['Feature']\n",
        "    density_corr = stats_df[stats_df['Feature'] == 'air_density_kgm3']['Spearman_r'].values[0]\n",
        "\n",
        "    print(f\"Top Predictor: {top_feature} (r={stats_df.iloc[0]['Spearman_r']:.4f})\")\n",
        "    if top_feature == 'theoretical_energy':\n",
        "        print(\"Observation: Theoretical energy outperforms raw wind speed, supporting physics-based feature engineering.\")\n",
        "\n",
        "\n",
        "    # CORRELATION HEATMAP\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Calculate full matrix for heatmap\n",
        "    full_corr = clean_df[features + [target]].corr(method='spearman')\n",
        "\n",
        "    # Create Custom Red-Green Palette\n",
        "    red_green_cmap = sns.diverging_palette(15, 125, s=75, l=50, center='light', as_cmap=True)\n",
        "\n",
        "    sns.heatmap(full_corr, annot=True, cmap=red_green_cmap, center=0, fmt=\".2f\", vmin=-1, vmax=1)\n",
        "\n",
        "    plt.title(f\"Spearman Correlation Matrix: {name}\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plot_path = os.path.join(PLOT_DIR, f\"Correlation_Matrix_{name.replace(' ', '_')}.png\")\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Saved Plot: {os.path.basename(plot_path)}\")\n",
        "\n",
        "    # DISTRIBUTION PLOTS\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Power Distribution\n",
        "    sns.histplot(clean_df['power_mw'], bins=50, kde=True, ax=axes[0], color=color, element=\"step\")\n",
        "    axes[0].set_title(f\"Power Output Distribution ({name})\")\n",
        "    axes[0].set_xlabel(\"Power (MW)\")\n",
        "    axes[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "    # Wind Speed Distribution\n",
        "    sns.histplot(clean_df['wind_speed_100m'], bins=50, kde=True, ax=axes[1], color=color, element=\"step\")\n",
        "    axes[1].set_title(f\"Wind Speed Distribution ({name})\")\n",
        "    axes[1].set_xlabel(\"Wind Speed (m/s)\")\n",
        "\n",
        "    # Air Density Distribution\n",
        "    sns.histplot(clean_df['air_density_kgm3'], bins=50, kde=True, ax=axes[2], color=color, element=\"step\")\n",
        "    axes[2].set_title(f\"Air Density Distribution ({name})\")\n",
        "    axes[2].set_xlabel(\"Density (kg/m³)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    dist_path = os.path.join(PLOT_DIR, f\"Distributions_{name.replace(' ', '_')}.png\")\n",
        "    plt.savefig(dist_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Saved Plot: {os.path.basename(dist_path)}\")\n",
        "\n",
        "\n",
        "# EXECUTION\n",
        "\n",
        "for name, f in FILES.items():\n",
        "    run_eda_suite(name, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znNzgnSMuQHj"
      },
      "source": [
        "## **Phase-02**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqwbrvThuQP0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import networkx as nx\n",
        "from statsmodels.tsa.stattools import adfuller, kpss, ccf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# VIBRANT COLOR PALETTE (For Lines/Distributions)\n",
        "ISLAND_COLORS = {\n",
        "    \"North Island\": \"#FF8C00\",  # Dark Orange\n",
        "    \"South Island\": \"#1E90FF\"   # Dodger Blue\n",
        "}\n",
        "\n",
        "# Hexbin Colormaps\n",
        "HEXBIN_MAP = {\n",
        "    \"North Island\": \"Oranges\",\n",
        "    \"South Island\": \"Blues\"\n",
        "}\n",
        "\n",
        "print(\"Starting Phase 2: EDA & Causal Structure (Red-Green Correction)\")\n",
        "print(f\"Plots directory: {PLOT_DIR}\")\n",
        "print(f\"Tables directory: {TABLE_DIR}\")\n",
        "\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def check_stationarity(series, name):\n",
        "    \"\"\"Runs ADF and KPSS tests.\"\"\"\n",
        "    series = series.dropna()\n",
        "    # ADF Test\n",
        "    adf_result = adfuller(series)\n",
        "    # KPSS Test\n",
        "    kpss_result = kpss(series, regression='c', nlags=\"auto\")\n",
        "\n",
        "    return {\n",
        "        \"Variable\": name,\n",
        "        \"ADF Statistic\": round(adf_result[0], 4),\n",
        "        \"ADF p-value\": round(adf_result[1], 4),\n",
        "        \"ADF Verdict\": \"Stationary\" if adf_result[1] < 0.05 else \"Non-Stationary\",\n",
        "        \"KPSS Statistic\": round(kpss_result[0], 4),\n",
        "        \"KPSS p-value\": round(kpss_result[1], 4),\n",
        "        \"KPSS Verdict\": \"Stationary\" if kpss_result[1] > 0.05 else \"Non-Stationary\"\n",
        "    }\n",
        "\n",
        "def plot_dag(region_name):\n",
        "    \"\"\"Creates and saves the Causal DAG.\"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Define Nodes\n",
        "    nodes = [\n",
        "        ('Seasonality', 'Confounder'),\n",
        "        ('Synoptic_State', 'Confounder'),\n",
        "        ('Wind_Speed', 'Mediator'),\n",
        "        ('Air_Density', 'Treatment'),\n",
        "        ('Power', 'Outcome')\n",
        "    ]\n",
        "    G.add_nodes_from([n[0] for n in nodes])\n",
        "\n",
        "    # Define Edges\n",
        "    edges = [\n",
        "        ('Seasonality', 'Synoptic_State'),\n",
        "        ('Seasonality', 'Air_Density'),\n",
        "        ('Synoptic_State', 'Wind_Speed'),\n",
        "        ('Synoptic_State', 'Air_Density'),\n",
        "        ('Wind_Speed', 'Power'),\n",
        "        ('Air_Density', 'Power')\n",
        "    ]\n",
        "    G.add_edges_from(edges)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    pos = {\n",
        "        'Seasonality': (0, 2),\n",
        "        'Synoptic_State': (0, 1),\n",
        "        'Wind_Speed': (-1, 0),\n",
        "        'Air_Density': (1, 0),\n",
        "        'Power': (0, -1)\n",
        "    }\n",
        "\n",
        "    node_colors = ['lightgray', 'lightblue', 'cyan', 'lightgreen', 'gold']\n",
        "\n",
        "    nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=4000,\n",
        "            edge_color='gray', width=2, arrowsize=20, font_weight='bold')\n",
        "\n",
        "    # Highlight Treatment Effect\n",
        "    nx.draw_networkx_edges(G, pos, edgelist=[('Air_Density', 'Power')],\n",
        "                           width=3, edge_color='red', arrowsize=25)\n",
        "\n",
        "    plt.title(f\"Causal DAG: {region_name}\")\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"DAG_{region_name.replace(' ', '_')}.png\"), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return \"Adjustment Set: {Synoptic_State, Seasonality, Wind_Speed}\"\n",
        "\n",
        "\n",
        "# MAIN EXECUTION LOOP\n",
        "for region, filename in FILES.items():\n",
        "    print(f\"Processing {region}\")\n",
        "    file_path = os.path.join(DATA_DIR, filename)\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        continue\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.set_index('timestamp')\n",
        "\n",
        "    # Filter curtailment for cleaner plots (Wind > 5 but Power < 0.1)\n",
        "    clean_df = df[~((df['wind_speed_100m'] > 5) & (df['power_mw'] <= 0.1))].copy()\n",
        "\n",
        "    # Get colors\n",
        "    island_color = ISLAND_COLORS[region]\n",
        "    hex_cmap = HEXBIN_MAP[region]\n",
        "\n",
        "    # Univariate Analysis\n",
        "    print(f\"[{region}] Generating Univariate Plots\")\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Power\n",
        "    sns.histplot(clean_df['power_mw'], bins=50, kde=True, ax=axes[0], color=island_color, element=\"step\")\n",
        "    axes[0].set_title(f\"Power Output Distribution ({region})\")\n",
        "    axes[0].set_xlabel(\"Power (MW)\")\n",
        "    axes[0].set_ylabel(\"Frequency\")\n",
        "\n",
        "    # Wind\n",
        "    sns.histplot(clean_df['wind_speed_100m'], bins=50, kde=True, ax=axes[1], color=island_color, element=\"step\")\n",
        "    axes[1].set_title(f\"Wind Speed Distribution ({region})\")\n",
        "    axes[1].set_xlabel(\"Wind Speed (m/s)\")\n",
        "\n",
        "    # Density\n",
        "    sns.histplot(clean_df['air_density_kgm3'], bins=50, kde=True, ax=axes[2], color=island_color, element=\"step\")\n",
        "    axes[2].set_title(f\"Air Density Distribution ({region})\")\n",
        "    axes[2].set_xlabel(\"Density (kg/m³)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"Univariate_Distributions_{region.replace(' ', '_')}.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Seasonality (Boxplots)\n",
        "    clean_df['Month'] = clean_df.index.month\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(x='Month', y='power_mw', data=clean_df, color=island_color)\n",
        "    plt.title(f\"Monthly Seasonality of Power Output: {region}\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Power (MW)\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"Seasonality_Boxplot_{region.replace(' ', '_')}.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Bivariate Analysis (Correlation)\n",
        "    print(f\"[{region}] Generating Correlation Matrix\")\n",
        "\n",
        "    cols = ['power_mw', 'wind_speed_100m', 'air_density_kgm3', 'temp_c', 'pressure_hpa']\n",
        "    corr = clean_df[cols].corr(method='spearman')\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Custom Red-Green Palette\n",
        "    red_green_cmap = sns.diverging_palette(15, 125, s=75, l=50, center='light', as_cmap=True)\n",
        "\n",
        "    sns.heatmap(corr, annot=True, cmap=red_green_cmap, center=0, fmt=\".2f\", vmin=-1, vmax=1)\n",
        "    plt.title(f\"Spearman Correlation Matrix: {region}\", pad=20)\n",
        "\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"Correlation_Matrix_{region.replace(' ', '_')}.png\"), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Bivariate Analysis (Power Curve)\n",
        "    print(f\"[{region}] Generating Power Curve\")\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hexbin(clean_df['wind_speed_100m'], clean_df['power_mw'], gridsize=50, cmap=hex_cmap, mincnt=1)\n",
        "    plt.colorbar(label='Count of Hours')\n",
        "    plt.title(f\"Empirical Power Curve: {region}\")\n",
        "    plt.xlabel(\"Wind Speed (m/s)\")\n",
        "    plt.ylabel(\"Power (MW)\")\n",
        "    plt.grid(True, alpha=0.2)\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"Power_Curve_{region.replace(' ', '_')}.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Bivariate Analysis (Polar Plot - Wind Direction)\n",
        "    print(f\"[{region}] Generating Polar Plot\")\n",
        "\n",
        "    # Bin wind direction (10 degree bins)\n",
        "    clean_df['dir_bin'] = (clean_df['wind_dir_100m'] // 10) * 10\n",
        "    dir_stats = clean_df.groupby('dir_bin')['power_mw'].mean()\n",
        "\n",
        "    # Convert to radians\n",
        "    theta = np.deg2rad(dir_stats.index)\n",
        "    radii = dir_stats.values\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    ax = plt.subplot(111, projection='polar')\n",
        "    ax.plot(theta, radii, color=island_color, linewidth=2)\n",
        "    ax.fill(theta, radii, alpha=0.3, color=island_color)\n",
        "\n",
        "    # Set North to top\n",
        "    ax.set_theta_zero_location(\"N\")\n",
        "    ax.set_theta_direction(-1)\n",
        "\n",
        "\n",
        "    plt.title(f\"Mean Power by Wind Direction: {region}\", pad=20)\n",
        "\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"Polar_Plot_{region.replace(' ', '_')}.png\"), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Lag Analysis\n",
        "    print(f\"[{region}] Performing Lag Analysis\")\n",
        "    lags = np.arange(0, 25)\n",
        "    wind = clean_df['wind_speed_100m']\n",
        "    power = clean_df['power_mw']\n",
        "    ccf_vals = ccf(wind, power, adjusted=False)[:25]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.stem(lags, ccf_vals, linefmt=island_color, markerfmt=island_color)\n",
        "    plt.title(f\"Lag Correlation (Wind vs Power): {region}\")\n",
        "    plt.xlabel(\"Lag (Hours)\")\n",
        "    plt.ylabel(\"Correlation Coefficient\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"Lag_Analysis_{region.replace(' ', '_')}.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Time Series Decomposition\n",
        "    print(f\"[{region}] Performing Time Series Decomposition\")\n",
        "    daily_df = clean_df.resample('D').mean().dropna()\n",
        "    res = seasonal_decompose(daily_df['power_mw'], model='additive', period=365)\n",
        "\n",
        "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
        "\n",
        "    res.observed.plot(ax=ax1, title='Observed', color=island_color, linewidth=1)\n",
        "    res.trend.plot(ax=ax2, title='Trend (365-Day Moving Avg)', color=island_color, linewidth=2)\n",
        "    res.seasonal.plot(ax=ax3, title='Seasonal Component', color=island_color, linewidth=1)\n",
        "    res.resid.plot(ax=ax4, title='Residuals', color='gray', alpha=0.5, linewidth=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"Decomposition_{region.replace(' ', '_')}.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # ACF/PACF\n",
        "    print(f\"[{region}] Generating ACF/PACF\")\n",
        "    residuals = res.resid.dropna()\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "    plot_acf(residuals, lags=40, ax=ax1, title=f\"ACF of Residuals: {region}\", color=island_color, vlines_kwargs={\"colors\": island_color})\n",
        "    plot_pacf(residuals, lags=40, ax=ax2, title=f\"PACF of Residuals: {region}\", color=island_color, vlines_kwargs={\"colors\": island_color})\n",
        "    plt.savefig(os.path.join(PLOT_DIR, f\"ACF_PACF_Residuals_{region.replace(' ', '_')}.png\"), dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Stationarity Tests\n",
        "    print(f\"[{region}] Running Stationarity Tests...\")\n",
        "    stat_results = []\n",
        "    stat_results.append(check_stationarity(clean_df['power_mw'], 'Power'))\n",
        "    stat_results.append(check_stationarity(clean_df['wind_speed_100m'], 'Wind Speed'))\n",
        "\n",
        "    stat_df = pd.DataFrame(stat_results)\n",
        "    stat_path = os.path.join(TABLE_DIR, f\"Stationarity_Tests_{region.replace(' ', '_')}.csv\")\n",
        "    stat_df.to_csv(stat_path, index=False)\n",
        "\n",
        "    # DAG & Adjustment Set\n",
        "    print(f\"[{region}] Generating Causal DAG\")\n",
        "    adj_set = plot_dag(region)\n",
        "\n",
        "    adj_path = os.path.join(TABLE_DIR, f\"Adjustment_Set_{region.replace(' ', '_')}.txt\")\n",
        "    print(\"Phase 2 Complete. All artifacts saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxyTHTibc80b"
      },
      "source": [
        "## **Phase-03**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, KFold, RandomizedSearchCV\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import r2_score\n",
        "from scipy import stats\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Causal Model Definition\n",
        "Y_COL = 'power_mw'\n",
        "T_COL = 'air_density_kgm3'\n",
        "\n",
        "# Excluded temp/pressure to avoid multicollinearity\n",
        "X_COLS = [\n",
        "    'wind_speed_100m', 'wind_cubed', 'turbulence_proxy', 'ramp_rate', 'wind_lag_1',\n",
        "    'wind_sin', 'wind_cos', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos'\n",
        "]\n",
        "\n",
        "# reporting scale\n",
        "EFFECT_SCALE = 0.1\n",
        "EFFECT_COL = \"Effect (MW per 0.1 kg/m3)\"\n",
        "\n",
        "\n",
        "all_results = []\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def tune_hyperparameters(X, y):\n",
        "    \"\"\"\n",
        "    Hyperparameter tuning using TimeSeriesSplit to prevent look-ahead bias.\n",
        "    \"\"\"\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.05, 0.1],\n",
        "        'subsample': [0.7, 0.8, 0.9],\n",
        "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
        "    }\n",
        "\n",
        "    xgb = XGBRegressor(n_jobs=-1, random_state=42)\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        xgb, param_grid, n_iter=10, cv=tscv,\n",
        "        scoring='neg_mean_squared_error', random_state=42, n_jobs=-1, verbose=0\n",
        "    )\n",
        "    search.fit(X, y)\n",
        "    return search.best_estimator_\n",
        "\n",
        "\n",
        "def run_dml_manual(df, model_y, model_t, n_folds=5):\n",
        "    \"\"\"\n",
        "    Manual DML implementation using Block K-Fold cross-fitting.\n",
        "    Ensures every data point is used for testing exactly once.\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=n_folds, shuffle=False)\n",
        "    X_data = df[X_COLS].values\n",
        "\n",
        "    residuals_y = np.zeros(len(df))\n",
        "    residuals_t = np.zeros(len(df))\n",
        "\n",
        "    # Cross-fitting loop\n",
        "    for train_idx, test_idx in kf.split(X_data):\n",
        "        X_train, X_test = X_data[train_idx], X_data[test_idx]\n",
        "        y_train = df[Y_COL].iloc[train_idx].values\n",
        "        y_test = df[Y_COL].iloc[test_idx].values\n",
        "        t_train = df[T_COL].iloc[train_idx].values\n",
        "        t_test = df[T_COL].iloc[test_idx].values\n",
        "\n",
        "        # Fit outcome model\n",
        "        model_y_fold = clone(model_y)\n",
        "        model_y_fold.fit(X_train, y_train)\n",
        "        residuals_y[test_idx] = y_test - model_y_fold.predict(X_test)\n",
        "\n",
        "        # Fit treatment model\n",
        "        model_t_fold = clone(model_t)\n",
        "        model_t_fold.fit(X_train, t_train)\n",
        "        residuals_t[test_idx] = t_test - model_t_fold.predict(X_test)\n",
        "\n",
        "    # DML: Regress outcome residuals on treatment residuals\n",
        "    coef, intercept, r_value, p_value, se = stats.linregress(residuals_t, residuals_y)\n",
        "\n",
        "    # 95% CI\n",
        "    ci_lower = coef - 1.96 * se\n",
        "    ci_upper = coef + 1.96 * se\n",
        "\n",
        "    # Calculate E-value (Sensitivity to unobserved confounding)\n",
        "    e_value = np.nan\n",
        "    if se > 0:\n",
        "        ci_lower_abs = abs(coef) - 1.96 * se\n",
        "        if ci_lower_abs > 0:\n",
        "            e_value = abs(coef) / abs(coef - ci_lower_abs)\n",
        "        else:\n",
        "            e_value = 1.0\n",
        "\n",
        "    return {\n",
        "        'Effect': coef,\n",
        "        'SE': se,\n",
        "        'P_Value': p_value,\n",
        "        'CI_Lower': ci_lower,\n",
        "        'CI_Upper': ci_upper,\n",
        "        'E_Value': round(e_value, 2) if not np.isnan(e_value) else np.nan,\n",
        "        'Residuals_Y': residuals_y,\n",
        "        'Residuals_T': residuals_t\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_diagnostics(df, model_y, model_t):\n",
        "    \"\"\"Calculate R2 diagnostics for nuisance models.\"\"\"\n",
        "    model_y.fit(df[X_COLS], df[Y_COL])\n",
        "    r2_y = r2_score(df[Y_COL], model_y.predict(df[X_COLS]))\n",
        "\n",
        "    model_t.fit(df[X_COLS], df[T_COL])\n",
        "    r2_t = r2_score(df[T_COL], model_t.predict(df[X_COLS]))\n",
        "\n",
        "    return r2_y, r2_t\n",
        "\n",
        "\n",
        "def process_subgroup(sub_df, region, subgroup_name, global_model_y, global_model_t):\n",
        "    \"\"\"Process single subgroup with adaptive retuning.\"\"\"\n",
        "    # Strict sample size check\n",
        "    if len(sub_df) < 5000:\n",
        "        return None\n",
        "\n",
        "    # Adaptive retuning for large subgroups\n",
        "    if len(sub_df) > 10000:\n",
        "        tune_sample = sub_df.sample(n=min(len(sub_df), 20000), random_state=42).sort_index()\n",
        "        model_y = tune_hyperparameters(tune_sample[X_COLS], tune_sample[Y_COL])\n",
        "        model_t = tune_hyperparameters(tune_sample[X_COLS], tune_sample[T_COL])\n",
        "    else:\n",
        "        model_y = clone(global_model_y)\n",
        "        model_t = clone(global_model_t)\n",
        "\n",
        "    # R2 diagnostics\n",
        "    r2_y, r2_t = calculate_diagnostics(sub_df, model_y, model_t)\n",
        "\n",
        "\n",
        "    # DML fitting\n",
        "    try:\n",
        "        dml_result = run_dml_manual(sub_df, model_y, model_t, n_folds=3)\n",
        "\n",
        "        # scale outputs to \"per 0.1 kg/m³\" ----\n",
        "        eff = dml_result['Effect'] * EFFECT_SCALE\n",
        "        se = dml_result['SE'] * EFFECT_SCALE\n",
        "        ci_l = dml_result['CI_Lower'] * EFFECT_SCALE\n",
        "        ci_u = dml_result['CI_Upper'] * EFFECT_SCALE\n",
        "        ---\n",
        "\n",
        "        return {\n",
        "            'Region': region,\n",
        "            'Subgroup': subgroup_name,\n",
        "            EFFECT_COL: eff,\n",
        "            'SE': se,\n",
        "            'P-Value': dml_result['P_Value'],\n",
        "            'CI_Lower': ci_l,\n",
        "            'CI_Upper': ci_u,\n",
        "            'N': len(sub_df),\n",
        "            'R2_Outcome': r2_y,\n",
        "            'R2_Treatment': r2_t,\n",
        "            'E_Value': dml_result['E_Value']\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error in {subgroup_name}: {str(e)[:50]}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def run_dml_analysis(region_name, df):\n",
        "    \"\"\"Main DML analysis pipeline.\"\"\"\n",
        "    print(f\"\\n{region_name}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    df = df.dropna(subset=[Y_COL, T_COL] + X_COLS)\n",
        "    print(f\"Dataset: {len(df):,} rows\")\n",
        "\n",
        "    # Global tuning\n",
        "    print(\"Tuning global models (50k sample)\")\n",
        "    if len(df) > 50000:\n",
        "        tune_df = df.sample(n=50000, random_state=42).sort_index()\n",
        "    else:\n",
        "        tune_df = df\n",
        "\n",
        "    global_model_y = tune_hyperparameters(tune_df[X_COLS], tune_df[Y_COL])\n",
        "    global_model_t = tune_hyperparameters(tune_df[X_COLS], tune_df[T_COL])\n",
        "\n",
        "    # Overall effect\n",
        "    print(\"Fitting overall effect\")\n",
        "    res = process_subgroup(df, region_name, \"Overall\", global_model_y, global_model_t)\n",
        "    if res:\n",
        "        all_results.append(res)\n",
        "        print(f\"Overall Effect: {res[EFFECT_COL]:.3f} MW per 0.1 kg/m³\")\n",
        "\n",
        "        # Residual plots\n",
        "        print(\"Generating residual diagnostics...\")\n",
        "        dml_res = run_dml_manual(df, global_model_y, global_model_t, n_folds=5)\n",
        "\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        color = '#FF8C00' if region_name == \"North Island\" else '#1E90FF'\n",
        "\n",
        "        sns.histplot(dml_res['Residuals_Y'], bins=50, kde=True, ax=ax[0], color=color, element=\"step\")\n",
        "        ax[0].set_title(f\"Outcome Model Residuals: {region_name}\", fontsize=12, fontweight='bold')\n",
        "        ax[0].set_xlabel(\"Residual Power (MW)\")\n",
        "        ax[0].grid(True, alpha=0.3)\n",
        "\n",
        "        sns.histplot(dml_res['Residuals_T'], bins=50, kde=True, ax=ax[1], color='gray', element=\"step\")\n",
        "        ax[1].set_title(f\"Treatment Model Residuals: {region_name}\", fontsize=12, fontweight='bold')\n",
        "        ax[1].set_xlabel(\"Residual Density (kg/m³)\")\n",
        "        ax[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\n",
        "            os.path.join(PLOT_DIR, f\"Residuals_Distribution_{region_name.replace(' ', '_')}.png\"),\n",
        "            dpi=300, bbox_inches='tight'\n",
        "        )\n",
        "        plt.close()\n",
        "\n",
        "    # Subgroup analysis\n",
        "    print(\"Running heterogeneous effects analysis...\")\n",
        "\n",
        "    season_map = {\n",
        "        12: 'Summer', 1: 'Summer', 2: 'Summer',\n",
        "        3: 'Autumn', 4: 'Autumn', 5: 'Autumn',\n",
        "        6: 'Winter', 7: 'Winter', 8: 'Winter',\n",
        "        9: 'Spring', 10: 'Spring', 11: 'Spring'\n",
        "    }\n",
        "    df['Season'] = df.index.month.map(season_map)\n",
        "    df['Is_Day'] = (df.index.hour >= 6) & (df.index.hour < 18)\n",
        "\n",
        "    subgroups = [\n",
        "        (\"Season: Summer\", df['Season'] == 'Summer'),\n",
        "        (\"Season: Autumn\", df['Season'] == 'Autumn'),\n",
        "        (\"Season: Winter\", df['Season'] == 'Winter'),\n",
        "        (\"Season: Spring\", df['Season'] == 'Spring'),\n",
        "        (\"Time: Day\", df['Is_Day']),\n",
        "        (\"Time: Night\", ~df['Is_Day']),\n",
        "        (\"Wind: Low (<6 m/s)\", df['wind_speed_100m'] < 6),\n",
        "        (\"Wind: Medium (6-10 m/s)\", (df['wind_speed_100m'] >= 6) & (df['wind_speed_100m'] < 10)),\n",
        "        (\"Wind: High (>10 m/s)\", df['wind_speed_100m'] >= 10),\n",
        "    ]\n",
        "\n",
        "    for name, mask in tqdm(subgroups, desc=\"Subgroups\", leave=False):\n",
        "        sub_df = df[mask].copy()\n",
        "        res = process_subgroup(sub_df, region_name, name, global_model_y, global_model_t)\n",
        "        if res:\n",
        "            all_results.append(res)\n",
        "\n",
        "# MAIN EXECUTION\n",
        "print(\"=\" * 60)\n",
        "print(\"PHASE 3: DOUBLE MACHINE LEARNING CAUSAL INFERENCE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for region, filename in FILES.items():\n",
        "    file_path = os.path.join(DATA_DIR, filename)\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"\\nLoading {region}...\")\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df = df.set_index('timestamp')\n",
        "\n",
        "        # Filter curtailment\n",
        "        df_clean = df[~((df['wind_speed_100m'] > 5) & (df['power_mw'] <= 0.1))].copy()\n",
        "        run_dml_analysis(region, df_clean)\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "# EXPORT & VISUALIZE\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXPORTING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "if not results_df.empty:\n",
        "    out_path = os.path.join(TABLE_DIR, \"DML_Causal_Effects_Summary.csv\")\n",
        "    results_df.to_csv(out_path, index=False)\n",
        "    print(f\"Results saved to: {out_path}\")\n",
        "\n",
        "    # Minimal Forest Plot Function\n",
        "    def plot_forest_minimal(df_res, title, filename):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        df_res = df_res.copy()\n",
        "\n",
        "        def clean_label(s):\n",
        "            if \":\" in s:\n",
        "                return s.split(\":\", 1)[1].strip()\n",
        "            return s\n",
        "\n",
        "        df_res['Label'] = df_res['Subgroup'].apply(clean_label)\n",
        "        df_res = df_res.sort_values(by=['Subgroup', 'Region'])\n",
        "\n",
        "        for region in sorted(df_res['Region'].unique()):\n",
        "            subset = df_res[df_res['Region'] == region]\n",
        "            color = '#FF8C00' if region == \"North Island\" else '#1E90FF'\n",
        "\n",
        "            y_pos = np.arange(len(subset))\n",
        "            offset = 0.15 if region == \"North Island\" else -0.15\n",
        "\n",
        "            plt.errorbar(\n",
        "                subset[EFFECT_COL],\n",
        "                y_pos + offset,\n",
        "                xerr=(subset[EFFECT_COL] - subset['CI_Lower']),\n",
        "                fmt='o', color=color, label=region, capsize=4,\n",
        "                alpha=0.9, markersize=8, linewidth=2\n",
        "            )\n",
        "\n",
        "            if region == \"North Island\":\n",
        "                plt.yticks(y_pos, subset['Label'])\n",
        "\n",
        "        plt.axvline(x=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "        plt.title(title, fontsize=12, fontweight='bold')\n",
        "        plt.xlabel(\"Causal Effect (MW per 0.1 kg/m³)\", fontsize=10)\n",
        "        plt.grid(True, alpha=0.2, axis='x')\n",
        "        plt.legend(loc='lower right', fontsize=10)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_path = os.path.join(PLOT_DIR, filename)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Saved plot: {filename}\")\n",
        "\n",
        "    # Generate Plots\n",
        "    seasonal = results_df[results_df['Subgroup'].str.contains(\"Season\", na=False)]\n",
        "    if not seasonal.empty:\n",
        "        plot_forest_minimal(seasonal, \"Seasonal Heterogeneity\", \"Forest_Plot_Seasonal.png\")\n",
        "\n",
        "    wind = results_df[results_df['Subgroup'].str.contains(\"Wind\", na=False)]\n",
        "    if not wind.empty:\n",
        "        plot_forest_minimal(wind, \"Wind Regime Heterogeneity\", \"Forest_Plot_WindRegime.png\")\n",
        "\n",
        "    diurnal = results_df[results_df['Subgroup'].str.contains(\"Time\", na=False)]\n",
        "    if not diurnal.empty:\n",
        "        plot_forest_minimal(diurnal, \"Diurnal Heterogeneity\", \"Forest_Plot_Diurnal.png\")\n",
        "\n",
        "    print(f\"\\nTotal results generated: {len(results_df)} rows\")\n",
        "else:\n",
        "    print(\"No results generated\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PHASE 3 COMPLETE\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "RsaCEDUoVg4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVQXKXM8lI-5"
      },
      "source": [
        "## **Phase-04**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEHKcKebcYlP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import clone\n",
        "from scipy import stats\n",
        "from statsmodels.stats.stattools import durbin_watson\n",
        "\n",
        "\n",
        "ISLAND_COLORS = {\n",
        "    \"North Island\": \"#FF8C00\",\n",
        "    \"South Island\": \"#1E90FF\"\n",
        "}\n",
        "\n",
        "# Causal Model Definition\n",
        "Y_COL = 'power_mw'\n",
        "T_COL = 'air_density_kgm3'\n",
        "# Excluded temp/pressure to avoid multicollinearity\n",
        "X_COLS = [\n",
        "    'wind_speed_100m', 'wind_cubed', 'turbulence_proxy', 'ramp_rate', 'wind_lag_1',\n",
        "    'wind_sin', 'wind_cos', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos'\n",
        "]\n",
        "\n",
        "robustness_results = []\n",
        "\n",
        "# MANUAL DML ENGINE (Consistent with Phase 3)\n",
        "def run_dml_manual(df, model_y, model_t, n_folds=5, treatment_col=T_COL):\n",
        "    \"\"\"\n",
        "    Manual DML implementation using K-fold cross-fitting.\n",
        "    Matches Phase 3 methodology exactly.\n",
        "    \"\"\"\n",
        "    # Block K-Fold (Respects Time Order, No Shuffling)\n",
        "    kf = KFold(n_splits=n_folds, shuffle=False)\n",
        "\n",
        "    # Prepare data matrices\n",
        "    X_data = df[X_COLS].values\n",
        "    y_data = df[Y_COL].values\n",
        "    t_data = df[treatment_col].values\n",
        "\n",
        "    residuals_y = np.zeros(len(df))\n",
        "    residuals_t = np.zeros(len(df))\n",
        "\n",
        "    # Cross-fitting loop\n",
        "    for train_idx, test_idx in kf.split(X_data):\n",
        "        X_train, X_test = X_data[train_idx], X_data[test_idx]\n",
        "        y_train, y_test = y_data[train_idx], y_data[test_idx]\n",
        "        t_train, t_test = t_data[train_idx], t_data[test_idx]\n",
        "\n",
        "        # Fit Outcome Model\n",
        "        m_y = clone(model_y)\n",
        "        m_y.fit(X_train, y_train)\n",
        "        residuals_y[test_idx] = y_test - m_y.predict(X_test)\n",
        "\n",
        "        # Fit Treatment Model\n",
        "        m_t = clone(model_t)\n",
        "        m_t.fit(X_train, t_train)\n",
        "        residuals_t[test_idx] = t_test - m_t.predict(X_test)\n",
        "\n",
        "    # Final Regression (Residual on Residual)\n",
        "    coef, intercept, r_value, p_value, se = stats.linregress(residuals_t, residuals_y)\n",
        "\n",
        "    return {\n",
        "        'Effect': coef,\n",
        "        'SE': se,\n",
        "        'P_Value': p_value,\n",
        "        'Residuals_Y': residuals_y, # Return for diagnostics\n",
        "        'Residuals_T': residuals_t\n",
        "    }\n",
        "\n",
        "# ROBUSTNESS SUITE\n",
        "def plot_physics_validation(df, region, color):\n",
        "    \"\"\"4.1 Physics Validation: Visualizing the Density Effect.\"\"\"\n",
        "    print(f\"Generating Physics Validation Plot...\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Filter for the \"Ramp\" section where density matters most (5-12 m/s)\n",
        "    subset = df[(df['wind_speed_100m'] >= 5) & (df['wind_speed_100m'] <= 12)].copy()\n",
        "\n",
        "    # Bin Density into High/Low\n",
        "    high_rho = subset['air_density_kgm3'].quantile(0.90)\n",
        "    low_rho = subset['air_density_kgm3'].quantile(0.10)\n",
        "\n",
        "    subset['Density_Bin'] = np.nan\n",
        "    subset.loc[subset['air_density_kgm3'] >= high_rho, 'Density_Bin'] = 'High Density (>90%)'\n",
        "    subset.loc[subset['air_density_kgm3'] <= low_rho, 'Density_Bin'] = 'Low Density (<10%)'\n",
        "\n",
        "    plot_data = subset.dropna(subset=['Density_Bin'])\n",
        "\n",
        "    # Plot\n",
        "    sns.lineplot(data=plot_data, x='wind_speed_100m', y='power_mw', hue='Density_Bin',\n",
        "                 style='Density_Bin', markers=True, dashes=False, palette='dark')\n",
        "\n",
        "    plt.title(f\"Physics Validation: Power Curve by Air Density ({region})\")\n",
        "    plt.xlabel(\"Wind Speed (m/s)\")\n",
        "    plt.ylabel(\"Power Output (MW)\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(title=\"Air Density Regime\")\n",
        "\n",
        "    save_path = os.path.join(PLOT_DIR, f\"Physics_Validation_{region.replace(' ', '_')}.png\")\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def run_robustness_checks(region, df):\n",
        "    print(f\"Running Robustness Suite for {region}...\")\n",
        "\n",
        "    # Base Learner (XGBoost - Consistent with Phase 3)\n",
        "    xgb_learner = XGBRegressor(n_estimators=100, max_depth=5, n_jobs=-1, random_state=42)\n",
        "\n",
        "    # Placebo Test\n",
        "    print(\"Running Placebo Test (Random Noise)...\")\n",
        "    # Create random noise variable\n",
        "    df['placebo_noise'] = np.random.normal(0, 1, size=len(df))\n",
        "\n",
        "    # Run DML treating Placebo as the Treatment\n",
        "    res_placebo = run_dml_manual(df, xgb_learner, xgb_learner, treatment_col='placebo_noise')\n",
        "\n",
        "    robustness_results.append({\n",
        "        'Region': region,\n",
        "        'Test_Type': 'Placebo',\n",
        "        'Specification': 'Random Noise Treatment',\n",
        "        'Effect': res_placebo['Effect'],\n",
        "        'SE': res_placebo['SE'],\n",
        "        'P_Value': res_placebo['P_Value']\n",
        "    })\n",
        "\n",
        "    # Temporal Stability (Date Split)\n",
        "    print(\"Running Temporal Stability Test (Pre/Post 2015)\")\n",
        "    split_date = '2015-01-01'\n",
        "    df_early = df[df.index < split_date]\n",
        "    df_late = df[df.index >= split_date]\n",
        "\n",
        "    # Early Era\n",
        "    if len(df_early) > 5000:\n",
        "        res_early = run_dml_manual(df_early, xgb_learner, xgb_learner)\n",
        "        robustness_results.append({\n",
        "            'Region': region,\n",
        "            'Test_Type': 'Temporal Stability',\n",
        "            'Specification': 'Early Era (2005-2014)',\n",
        "            'Effect': res_early['Effect'],\n",
        "            'SE': res_early['SE'],\n",
        "            'P_Value': res_early['P_Value']\n",
        "        })\n",
        "\n",
        "    # Late Era\n",
        "    if len(df_late) > 5000:\n",
        "        res_late = run_dml_manual(df_late, xgb_learner, xgb_learner)\n",
        "        robustness_results.append({\n",
        "            'Region': region,\n",
        "            'Test_Type': 'Temporal Stability',\n",
        "            'Specification': 'Late Era (2015-2024)',\n",
        "            'Effect': res_late['Effect'],\n",
        "            'SE': res_late['SE'],\n",
        "            'P_Value': res_late['P_Value']\n",
        "        })\n",
        "\n",
        "    # Learner Robustness\n",
        "    print(\"Running Learner Robustness (RF & OLS)\")\n",
        "\n",
        "    # Random Forest\n",
        "    rf_learner = RandomForestRegressor(n_estimators=50, max_depth=7, n_jobs=-1, random_state=42)\n",
        "    res_rf = run_dml_manual(df, rf_learner, rf_learner)\n",
        "\n",
        "    robustness_results.append({\n",
        "        'Region': region,\n",
        "        'Test_Type': 'Learner Robustness',\n",
        "        'Specification': 'Random Forest',\n",
        "        'Effect': res_rf['Effect'],\n",
        "        'SE': res_rf['SE'],\n",
        "        'P_Value': res_rf['P_Value']\n",
        "    })\n",
        "\n",
        "    # Linear Regression\n",
        "    ols_learner = LinearRegression()\n",
        "    res_ols = run_dml_manual(df, ols_learner, ols_learner)\n",
        "\n",
        "    robustness_results.append({\n",
        "        'Region': region,\n",
        "        'Test_Type': 'Learner Robustness',\n",
        "        'Specification': 'Linear Regression (OLS)',\n",
        "        'Effect': res_ols['Effect'],\n",
        "        'SE': res_ols['SE'],\n",
        "        'P_Value': res_ols['P_Value']\n",
        "    })\n",
        "\n",
        "    # Residual Diagnostics (Durbin-Watson)\n",
        "    print(\"Calculating Residual Autocorrelation (Durbin-Watson)...\")\n",
        "    # We use the residuals from the Main XGBoost\n",
        "    res_main = run_dml_manual(df, xgb_learner, xgb_learner)\n",
        "\n",
        "    # Durbin-Watson on the Final DML Residuals (Y_resid - theta * T_resid)\n",
        "    dw_stat = durbin_watson(res_main['Residuals_Y'])\n",
        "\n",
        "    robustness_results.append({\n",
        "        'Region': region,\n",
        "        'Test_Type': 'Diagnostics',\n",
        "        'Specification': 'Durbin-Watson Statistic',\n",
        "        'Effect': dw_stat,\n",
        "        'SE': np.nan,\n",
        "        'P_Value': np.nan\n",
        "    })\n",
        "\n",
        "\n",
        "# EXECUTION LOOP\n",
        "for region, filename in FILES.items():\n",
        "    file_path = os.path.join(DATA_DIR, filename)\n",
        "    if os.path.exists(file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df = df.set_index('timestamp')\n",
        "\n",
        "        # Filter curtailment\n",
        "        df_clean = df[~((df['wind_speed_100m'] > 5) & (df['power_mw'] <= 0.1))].copy()\n",
        "        df_clean = df_clean.dropna(subset=[Y_COL, T_COL] + X_COLS)\n",
        "\n",
        "        # Physics Plot\n",
        "        plot_physics_validation(df_clean, region, ISLAND_COLORS[region])\n",
        "\n",
        "        # Run Suite\n",
        "        run_robustness_checks(region, df_clean)\n",
        "\n",
        "\n",
        "# EXPORT RESULTS\n",
        "results_df = pd.DataFrame(robustness_results)\n",
        "out_path = os.path.join(TABLE_DIR, \"Robustness_Validation_Summary.csv\")\n",
        "results_df.to_csv(out_path, index=False)\n",
        "\n",
        "print(\"\\nPhase 4 Complete.\")\n",
        "print(f\"Robustness Table saved to: {out_path}\")\n",
        "print(f\"Validation Plots saved to: {PLOT_DIR}\")\n",
        "\n",
        "# Print Summary\n",
        "print(\"\\nSummary of Robustness Checks:\")\n",
        "print(results_df[['Region', 'Test_Type', 'Specification', 'Effect']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SOUTH ISLAND ERA-STRATIFIED VALIDATION & DIAGNOSTICS**"
      ],
      "metadata": {
        "id": "shUeZGW71xDn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDkdihKM8gDh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from scipy import stats\n",
        "from scipy.stats import spearmanr, kruskal\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "Y_COL = 'power_mw'\n",
        "T_COL = 'air_density_kgm3'\n",
        "X_COLS = [\n",
        "    'wind_speed_100m', 'wind_cubed', 'turbulence_proxy', 'ramp_rate', 'wind_lag_1',\n",
        "    'wind_sin', 'wind_cos', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos'\n",
        "]\n",
        "\n",
        "\n",
        "# HELPER: Load & Split Data\n",
        "def load_and_split_south_island():\n",
        "    \"\"\"Load South Island data and split by 2015 date.\"\"\"\n",
        "    file_path = os.path.join(DATA_DIR, \"ENHANCED_DATASET_SOUTH_ISLAND.csv\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df = df.set_index('timestamp')\n",
        "\n",
        "    # Filter curtailment\n",
        "    df_clean = df[~((df['wind_speed_100m'] > 5) & (df['power_mw'] <= 0.1))].copy()\n",
        "    df_clean = df_clean.dropna(subset=[Y_COL, T_COL] + X_COLS)\n",
        "\n",
        "    # Split by date\n",
        "    split_date = '2015-01-01'\n",
        "    df_early = df_clean[df_clean.index < split_date]\n",
        "    df_late = df_clean[df_clean.index >= split_date]\n",
        "\n",
        "    print(f\"Early data: {df_early.index.min()} to {df_early.index.max()} (n={len(df_early):,})\")\n",
        "    print(f\"Late data:  {df_late.index.min()} to {df_late.index.max()} (n={len(df_late):,})\")\n",
        "\n",
        "    return df_clean, df_early, df_late\n",
        "\n",
        "\n",
        "\n",
        "# ERA DIAGNOSTICS TABLE\n",
        "def diagnostic_table_by_era(df_early, df_late, df_all):\n",
        "    \"\"\"Compare key statistics across eras.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SOUTH ISLAND ERA DIAGNOSTICS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    diagnostics = []\n",
        "\n",
        "    for era_name, df_era in [(\"Early (2005-2014)\", df_early),\n",
        "                              (\"Late (2015-2024)\", df_late),\n",
        "                              (\"All (2005-2024)\", df_all)]:\n",
        "\n",
        "        n = len(df_era)\n",
        "        power_mean = df_era[Y_COL].mean()\n",
        "        power_std = df_era[Y_COL].std()\n",
        "        power_min = df_era[Y_COL].min()\n",
        "        power_max = df_era[Y_COL].max()\n",
        "\n",
        "        density_mean = df_era[T_COL].mean()\n",
        "        density_std = df_era[T_COL].std()\n",
        "        wind_mean = df_era['wind_speed_100m'].mean()\n",
        "\n",
        "        # Seasonal composition\n",
        "        season_map = {\n",
        "            12: 'Summer', 1: 'Summer', 2: 'Summer',\n",
        "            3: 'Autumn', 4: 'Autumn', 5: 'Autumn',\n",
        "            6: 'Winter', 7: 'Winter', 8: 'Winter',\n",
        "            9: 'Spring', 10: 'Spring', 11: 'Spring'\n",
        "        }\n",
        "        df_era_copy = df_era.copy()\n",
        "        df_era_copy['Season'] = df_era_copy.index.month.map(season_map)\n",
        "        summer_pct = (df_era_copy['Season'] == 'Summer').sum() / n * 100\n",
        "\n",
        "        diagnostics.append({\n",
        "            'Era': era_name,\n",
        "            'N': f\"{n:,}\",\n",
        "            'Power_Mean_MW': f\"{power_mean:.2f}\",\n",
        "            'Power_Range_MW': f\"{power_min:.1f}–{power_max:.1f}\",\n",
        "            'Density_Mean': f\"{density_mean:.4f}\",\n",
        "            'Density_Std': f\"{density_std:.4f}\",\n",
        "            'Wind_Mean_ms': f\"{wind_mean:.2f}\",\n",
        "            'Summer_pct': f\"{summer_pct:.1f}%\"\n",
        "        })\n",
        "\n",
        "    diag_df = pd.DataFrame(diagnostics)\n",
        "    print(diag_df.to_string(index=False))\n",
        "\n",
        "    # Save\n",
        "    diag_path = os.path.join(TABLE_DIR, \"South_Island_Era_Diagnostics.csv\")\n",
        "    diag_df.to_csv(diag_path, index=False)\n",
        "    print(f\"\\nSaved to: {diag_path}\")\n",
        "\n",
        "    return diag_df\n",
        "\n",
        "# CONFOUNDER BALANCE BY ERA\n",
        "def confounder_balance_by_era(df_early, df_late):\n",
        "    \"\"\"Check if confounders have different distributions by era.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CONFOUNDER BALANCE BY ERA (Kruskal-Wallis Test)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    balance_results = []\n",
        "\n",
        "    for col in X_COLS:\n",
        "        # Kruskal-Wallis test\n",
        "        stat, p_val = kruskal(df_early[col].dropna(), df_late[col].dropna())\n",
        "\n",
        "        mean_early = df_early[col].mean()\n",
        "        mean_late = df_late[col].mean()\n",
        "        pct_diff = ((mean_late - mean_early) / abs(mean_early)) * 100 if mean_early != 0 else 0\n",
        "\n",
        "        balance_results.append({\n",
        "            'Confounder': col,\n",
        "            'Mean_Early': f\"{mean_early:.4f}\",\n",
        "            'Mean_Late': f\"{mean_late:.4f}\",\n",
        "            'Pct_Diff': f\"{pct_diff:+.2f}%\",\n",
        "            'P_Value': f\"{p_val:.4f}\",\n",
        "            'Sig': '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else 'NS'\n",
        "        })\n",
        "\n",
        "    balance_df = pd.DataFrame(balance_results)\n",
        "    print(balance_df.to_string(index=False))\n",
        "\n",
        "    # Save\n",
        "    balance_path = os.path.join(TABLE_DIR, \"South_Island_Confounder_Balance.csv\")\n",
        "    balance_df.to_csv(balance_path, index=False)\n",
        "    print(f\"\\nSaved to: {balance_path}\")\n",
        "\n",
        "# CORRELATION STRUCTURE BY ERA\n",
        "def correlation_by_era(df_early, df_late):\n",
        "    \"\"\"Compare correlations between Density and Power by era.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PARTIAL CORRELATION: Density → Power by Era\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    corr_results = []\n",
        "\n",
        "    for era_name, df_era in [(\"Early (2005-2014)\", df_early),\n",
        "                              (\"Late (2015-2024)\", df_late)]:\n",
        "\n",
        "        # Raw correlation\n",
        "        raw_corr, raw_p = spearmanr(df_era[T_COL], df_era[Y_COL])\n",
        "\n",
        "        # Control for wind speed (primary confounder)\n",
        "        lr_y = LinearRegression().fit(df_era[['wind_speed_100m']], df_era[Y_COL])\n",
        "        lr_t = LinearRegression().fit(df_era[['wind_speed_100m']], df_era[T_COL])\n",
        "\n",
        "        y_resid = df_era[Y_COL] - lr_y.predict(df_era[['wind_speed_100m']])\n",
        "        t_resid = df_era[T_COL] - lr_t.predict(df_era[['wind_speed_100m']])\n",
        "\n",
        "        partial_corr, partial_p = spearmanr(t_resid, y_resid)\n",
        "\n",
        "        corr_results.append({\n",
        "            'Era': era_name,\n",
        "            'Raw_Corr': f\"{raw_corr:.4f}\",\n",
        "            'Raw_P': f\"{raw_p:.4f}\",\n",
        "            'Partial_Corr': f\"{partial_corr:.4f}\",\n",
        "            'Partial_P': f\"{partial_p:.4f}\",\n",
        "            'Sign_Flip': 'YES' if (raw_corr * partial_corr < 0) else 'NO'\n",
        "        })\n",
        "\n",
        "    corr_df = pd.DataFrame(corr_results)\n",
        "    print(corr_df.to_string(index=False))\n",
        "\n",
        "    # Save\n",
        "    corr_path = os.path.join(TABLE_DIR, \"South_Island_Correlation_by_Era.csv\")\n",
        "    corr_df.to_csv(corr_path, index=False)\n",
        "    print(f\"\\n✓ Saved to: {corr_path}\")\n",
        "\n",
        "# SEASONAL COMPOSITION ANALYSIS\n",
        "def seasonal_analysis(df_early, df_late, df_all):\n",
        "    \"\"\"Check if seasonal composition differs between eras.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SEASONAL COMPOSITION BY ERA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    season_map = {\n",
        "        12: 'Summer', 1: 'Summer', 2: 'Summer',\n",
        "        3: 'Autumn', 4: 'Autumn', 5: 'Autumn',\n",
        "        6: 'Winter', 7: 'Winter', 8: 'Winter',\n",
        "        9: 'Spring', 10: 'Spring', 11: 'Spring'\n",
        "    }\n",
        "\n",
        "    seasonal_results = []\n",
        "\n",
        "    for era_name, df_era in [(\"Early (2005-2014)\", df_early),\n",
        "                              (\"Late (2015-2024)\", df_late),\n",
        "                              (\"All (2005-2024)\", df_all)]:\n",
        "\n",
        "        df_era_copy = df_era.copy()\n",
        "        df_era_copy['Season'] = df_era_copy.index.month.map(season_map)\n",
        "\n",
        "        for season in ['Summer', 'Autumn', 'Winter', 'Spring']:\n",
        "            season_data = df_era_copy[df_era_copy['Season'] == season]\n",
        "\n",
        "            if len(season_data) > 0:\n",
        "                pct = len(season_data) / len(df_era) * 100\n",
        "                seasonal_results.append({\n",
        "                    'Era': era_name,\n",
        "                    'Season': season,\n",
        "                    'Pct_of_Era': f\"{pct:.1f}%\",\n",
        "                    'N': f\"{len(season_data):,}\"\n",
        "                })\n",
        "\n",
        "    seasonal_df = pd.DataFrame(seasonal_results)\n",
        "    print(seasonal_df.to_string(index=False))\n",
        "\n",
        "    # Chi-square test\n",
        "    early_seasonal = df_early.copy()\n",
        "    early_seasonal['Season'] = early_seasonal.index.month.map(season_map)\n",
        "    early_counts = early_seasonal['Season'].value_counts().sort_index().values  # FIX: Add .values\n",
        "\n",
        "    late_seasonal = df_late.copy()\n",
        "    late_seasonal['Season'] = late_seasonal.index.month.map(season_map)\n",
        "    late_counts = late_seasonal['Season'].value_counts().sort_index().values  # FIX: Add .values\n",
        "\n",
        "    chi2, p = stats.chisquare(\n",
        "        f_obs=late_counts,\n",
        "        f_exp=(early_counts / early_counts.sum() * late_counts.sum())\n",
        "    )\n",
        "\n",
        "    print(f\"\\nChi-square test for seasonal distribution difference:\")\n",
        "    print(f\"  χ² = {chi2:.3f}, p-value = {p:.4f}\")\n",
        "\n",
        "    # Save\n",
        "    seasonal_path = os.path.join(TABLE_DIR, \"South_Island_Seasonal_by_Era.csv\")\n",
        "    seasonal_df.to_csv(seasonal_path, index=False)\n",
        "    print(f\"Saved to: {seasonal_path}\")\n",
        "\n",
        "# VISUALIZATION: Density vs Power by Era\n",
        "def plot_density_power_by_era(df_early, df_late):\n",
        "    \"\"\"Scatterplot of Density vs Power, colored by era.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"GENERATING VISUALIZATION: Density vs Power by Era\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Early Era\n",
        "    axes[0].scatter(df_early[T_COL], df_early[Y_COL], alpha=0.3, s=10, color='blue')\n",
        "    z_early = np.polyfit(df_early[T_COL].dropna(), df_early[Y_COL].dropna(), 1)\n",
        "    p_early = np.poly1d(z_early)\n",
        "    x_range = np.linspace(df_early[T_COL].min(), df_early[T_COL].max(), 100)\n",
        "    axes[0].plot(x_range, p_early(x_range), 'b-', linewidth=3, label=f'Slope: {z_early[0]:.1f} MW/(kg/m³)')\n",
        "    axes[0].set_xlabel('Air Density (kg/m³)', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_ylabel('Power (MW)', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_title('Early Era (2005-2014)', fontsize=13, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Late Era\n",
        "    axes[1].scatter(df_late[T_COL], df_late[Y_COL], alpha=0.3, s=10, color='red')\n",
        "    z_late = np.polyfit(df_late[T_COL].dropna(), df_late[Y_COL].dropna(), 1)\n",
        "    p_late = np.poly1d(z_late)\n",
        "    x_range = np.linspace(df_late[T_COL].min(), df_late[T_COL].max(), 100)\n",
        "    axes[1].plot(x_range, p_late(x_range), 'r-', linewidth=3, label=f'Slope: {z_late[0]:.1f} MW/(kg/m³)')\n",
        "    axes[1].set_xlabel('Air Density (kg/m³)', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_ylabel('Power (MW)', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_title('Late Era (2015-2024)', fontsize=13, fontweight='bold')\n",
        "    axes[1].legend(fontsize=10)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Overlay\n",
        "    axes[2].scatter(df_early[T_COL], df_early[Y_COL], alpha=0.2, s=10, color='blue', label='Early')\n",
        "    axes[2].scatter(df_late[T_COL], df_late[Y_COL], alpha=0.2, s=10, color='red', label='Late')\n",
        "    axes[2].plot(x_range, p_early(x_range), 'b-', linewidth=3, label=f'Early: {z_early[0]:.1f}')\n",
        "    axes[2].plot(x_range, p_late(x_range), 'r-', linewidth=3, label=f'Late: {z_late[0]:.1f}')\n",
        "    axes[2].set_xlabel('Air Density (kg/m³)', fontsize=12, fontweight='bold')\n",
        "    axes[2].set_ylabel('Power (MW)', fontsize=12, fontweight='bold')\n",
        "    axes[2].set_title('Overlay Comparison', fontsize=13, fontweight='bold')\n",
        "    axes[2].legend(fontsize=10)\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(PLOT_DIR, \"South_Island_Density_Power_by_Era.png\")\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Saved to: {plot_path}\")\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SOUTH ISLAND ERA-STRATIFIED VALIDATION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Load data\n",
        "    df_all, df_early, df_late = load_and_split_south_island()\n",
        "\n",
        "    # Run diagnostics\n",
        "    diagnostic_table_by_era(df_early, df_late, df_all)\n",
        "    confounder_balance_by_era(df_early, df_late)\n",
        "    correlation_by_era(df_early, df_late)\n",
        "    seasonal_analysis(df_early, df_late, df_all)\n",
        "    plot_density_power_by_era(df_early, df_late)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SOUTH ISLAND ERA-STRATIFIED VALIDATION COMPLETE\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeqeL2VVBxPW"
      },
      "source": [
        "## **Phase-05**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dZyIwyy_zBM"
      },
      "outputs": [],
      "source": [
        "# SHAP INTERPRETABILITY\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shap\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Global styling\n",
        "plt.rcParams.update({\n",
        "    \"figure.dpi\": 180,\n",
        "    \"savefig.dpi\": 300,\n",
        "    \"font.size\": 12,\n",
        "    \"axes.titlesize\": 14,\n",
        "    \"axes.titlepad\": 18,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"xtick.labelsize\": 11,\n",
        "    \"ytick.labelsize\": 11,\n",
        "    \"figure.facecolor\": \"white\",\n",
        "    \"axes.facecolor\": \"white\",\n",
        "})\n",
        "\n",
        "Y_COL = \"power_mw\"\n",
        "T_COL = \"air_density_kgm3\"\n",
        "X_COLS = [\n",
        "    \"wind_speed_100m\", \"wind_cubed\", \"turbulence_proxy\", \"ramp_rate\", \"wind_lag_1\",\n",
        "    \"wind_sin\", \"wind_cos\", \"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\"\n",
        "]\n",
        "\n",
        "SEASON_MAP = {\n",
        "    12: \"Summer\", 1: \"Summer\", 2: \"Summer\",\n",
        "    3: \"Autumn\", 4: \"Autumn\", 5: \"Autumn\",\n",
        "    6: \"Winter\", 7: \"Winter\", 8: \"Winter\",\n",
        "    9: \"Spring\", 10: \"Spring\", 11: \"Spring\"\n",
        "}\n",
        "\n",
        "\n",
        "# Helpers\n",
        "def tune_hyperparameters(X, y):\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"max_depth\": [3, 5, 7],\n",
        "        \"learning_rate\": [0.05, 0.1],\n",
        "        \"subsample\": [0.8, 0.9],\n",
        "        \"colsample_bytree\": [0.8, 0.9],\n",
        "    }\n",
        "    model = XGBRegressor(n_jobs=-1, random_state=42)\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    search = RandomizedSearchCV(\n",
        "        model,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=5,\n",
        "        cv=tscv,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbose=0,\n",
        "    )\n",
        "    search.fit(X, y)\n",
        "    return search.best_estimator_\n",
        "\n",
        "def save_fig(path, fig=None, pad=0.60, rect=(0.03, 0.03, 0.97, 0.92)):\n",
        "    \"\"\"\n",
        "    Unified save method with extra padding to avoid clipping (titles/colorbars).\n",
        "    rect=(left, bottom, right, top) reserved area for tight_layout.\n",
        "    \"\"\"\n",
        "    if fig is None:\n",
        "        fig = plt.gcf()\n",
        "    try:\n",
        "        fig.tight_layout(rect=list(rect))\n",
        "    except Exception:\n",
        "        pass\n",
        "    fig.savefig(path, bbox_inches=\"tight\", pad_inches=pad, facecolor=\"white\")\n",
        "    plt.close(fig)\n",
        "\n",
        "def stratified_sample(df, y_col, max_n=10000, n_tail=500, seed=42):\n",
        "    \"\"\"Tail-aware sampling: top + bottom + random remainder.\"\"\"\n",
        "    if len(df) <= max_n:\n",
        "        return df.copy()\n",
        "\n",
        "    top_power = df.nlargest(n_tail, y_col)\n",
        "    low_power = df.nsmallest(n_tail, y_col)\n",
        "    random_n = max_n - 2 * n_tail\n",
        "    random_sample = df.sample(n=random_n, random_state=seed)\n",
        "\n",
        "    out = pd.concat([top_power, low_power, random_sample]).drop_duplicates()\n",
        "    if len(out) > max_n:\n",
        "        out = out.sample(n=max_n, random_state=seed)\n",
        "    return out\n",
        "\n",
        "# Phase-05 runner\n",
        "def run_shap_analysis(region, filename):\n",
        "    print(f\"\\n{'='*70}\\nSHAP Analysis: {region}\\n{'='*70}\")\n",
        "\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"File not found: {path}\")\n",
        "        return\n",
        "\n",
        "    # Load & prep\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "    df = df.set_index(\"timestamp\").sort_index()\n",
        "    df = df.dropna(subset=[Y_COL, T_COL] + X_COLS)\n",
        "\n",
        "    # Season (before sampling)\n",
        "    df[\"Season\"] = df.index.month.map(SEASON_MAP)\n",
        "\n",
        "    # Stratified sampling\n",
        "    shap_df = stratified_sample(df, Y_COL, max_n=10000, n_tail=500, seed=42).reset_index(drop=True)\n",
        "\n",
        "    X = shap_df[X_COLS]\n",
        "    y = shap_df[Y_COL]\n",
        "\n",
        "    # Train\n",
        "    print(\"Training outcome model (XGBoost)\")\n",
        "    model_y = tune_hyperparameters(X, y)\n",
        "\n",
        "    # ---- SHAP\n",
        "    print(\"Computing SHAP values\")\n",
        "    explainer = shap.TreeExplainer(model_y)\n",
        "    shap_values = explainer(X)\n",
        "\n",
        "\n",
        "    # Summary plot (robust)\n",
        "    print(\"Saving summary plot\")\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    shap.summary_plot(\n",
        "        shap_values, X,\n",
        "        show=False,\n",
        "        plot_type=\"dot\",\n",
        "        cmap=plt.get_cmap(\"autumn\" if region == \"North Island\" else \"winter\"),\n",
        "    )\n",
        "    plt.title(f\"SHAP Summary: Drivers of Power Generation\\n{region}\",\n",
        "              fontsize=16, fontweight=\"bold\", pad=28)\n",
        "    save_fig(os.path.join(PLOT_DIR, f\"SHAP_Summary_Outcome_{region.replace(' ', '_')}.png\"),\n",
        "             pad=0.65, rect=(0.03, 0.03, 0.97, 0.90))\n",
        "\n",
        "    # Dependence plot\n",
        "    print(\"Saving dependence plot\")\n",
        "    plt.figure(figsize=(12, 9))\n",
        "    shap.plots.scatter(\n",
        "        shap_values[:, \"wind_speed_100m\"],\n",
        "        color=shap_values[:, \"turbulence_proxy\"],\n",
        "        show=False\n",
        "    )\n",
        "    plt.title(f\"SHAP Dependence: Wind Speed Impact on Power\\n{region}\",\n",
        "              fontsize=16, fontweight=\"bold\", pad=26)\n",
        "    save_fig(os.path.join(PLOT_DIR, f\"SHAP_Dependence_Wind_{region.replace(' ', '_')}.png\"),\n",
        "             pad=0.65, rect=(0.03, 0.03, 0.97, 0.90))\n",
        "\n",
        "    # Seasonal comparison\n",
        "    print(\"Saving seasonal comparison (Summer vs Winter)\")\n",
        "    summer_idx = np.where((shap_df[\"Season\"] == \"Summer\").values)[0]\n",
        "    winter_idx = np.where((shap_df[\"Season\"] == \"Winter\").values)[0]\n",
        "\n",
        "    if (len(summer_idx) > 50) and (len(winter_idx) > 50):\n",
        "        fig, axes = plt.subplots(\n",
        "            1, 2,\n",
        "            figsize=(28, 9),\n",
        "            gridspec_kw={\"wspace\": 0.95}\n",
        "        )\n",
        "\n",
        "        # Summer\n",
        "        plt.sca(axes[0])\n",
        "        shap.plots.bar(shap_values[summer_idx], max_display=10, show=False)\n",
        "        axes[0].set_title(\n",
        "            f\"Feature Importance\\nSummer ({region})\\nN={len(summer_idx)}\",\n",
        "            fontsize=13, fontweight=\"bold\", pad=16\n",
        "        )\n",
        "\n",
        "        # Winter\n",
        "        plt.sca(axes[1])\n",
        "        shap.plots.bar(shap_values[winter_idx], max_display=10, show=False)\n",
        "        axes[1].set_title(\n",
        "            f\"Feature Importance\\nWinter ({region})\\nN={len(winter_idx)}\",\n",
        "            fontsize=13, fontweight=\"bold\", pad=16\n",
        "        )\n",
        "\n",
        "        fig.suptitle(\n",
        "            f\"Seasonal Feature Importance Comparison\\n{region}\",\n",
        "            fontsize=18, fontweight=\"bold\", y=0.98\n",
        "        )\n",
        "\n",
        "        # Explicit spacing control:\n",
        "        fig.subplots_adjust(\n",
        "            top=0.82,\n",
        "            bottom=0.10,\n",
        "            left=0.08,\n",
        "            right=0.98,\n",
        "            wspace=0.95\n",
        "        )\n",
        "\n",
        "        out_path = os.path.join(PLOT_DIR, f\"SHAP_Seasonal_Compare_{region.replace(' ', '_')}.png\")\n",
        "        fig.savefig(out_path, dpi=300, bbox_inches=\"tight\", pad_inches=0.70, facecolor=\"white\")\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        print(f\"Skipping seasonal plot (summer={len(summer_idx)}, winter={len(winter_idx)})\")\n",
        "\n",
        "\n",
        "    # Waterfall: High power event\n",
        "    print(\"Saving waterfall (high power event)\")\n",
        "    high_idx = int(np.argmax(y.values))\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    shap.plots.waterfall(shap_values[high_idx], max_display=12, show=False)\n",
        "    plt.title(f\"Local Explanation: High Power Event\\n{region} | Power={y.iloc[high_idx]:.1f} MW\",\n",
        "              fontsize=16, fontweight=\"bold\", pad=28)\n",
        "    save_fig(os.path.join(PLOT_DIR, f\"SHAP_Waterfall_High_{region.replace(' ', '_')}.png\"),\n",
        "             pad=0.70, rect=(0.03, 0.03, 0.97, 0.90))\n",
        "\n",
        "    # Waterfall: Low power event\n",
        "    print(\"Saving waterfall (low power event)\")\n",
        "    low_idx = int(np.argmin(y.values))\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    shap.plots.waterfall(shap_values[low_idx], max_display=12, show=False)\n",
        "    plt.title(f\"Local Explanation: Low Power Event\\n{region} | Power={y.iloc[low_idx]:.1f} MW\",\n",
        "              fontsize=16, fontweight=\"bold\", pad=28)\n",
        "    save_fig(os.path.join(PLOT_DIR, f\"SHAP_Waterfall_Low_{region.replace(' ', '_')}.png\"),\n",
        "             pad=0.70, rect=(0.03, 0.03, 0.97, 0.90))\n",
        "\n",
        "    print(f\"Done: {region} (5 plots saved)\")\n",
        "\n",
        "# Execute\n",
        "print(\"=\"*80)\n",
        "print(\"PHASE 05: SHAP ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for region_name, fname in FILES.items():\n",
        "    run_shap_analysis(region_name, fname)\n",
        "\n",
        "print(\"\\nAll Phase-05 plots saved to:\")\n",
        "print(PLOT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOnftulmRlb1"
      },
      "source": [
        "## **Phase-06**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alzP8Oi0Rnxb"
      },
      "outputs": [],
      "source": [
        "# PHASE 06: Sensitivity + Time-series Robust Inference + Overlap Diagnostics\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import r2_score\n",
        "from scipy import stats\n",
        "\n",
        "ISLAND_COLORS = {\n",
        "    \"North Island\": \"#FF8C00\",\n",
        "    \"South Island\": \"#1E90FF\"\n",
        "}\n",
        "\n",
        "Y_COL = \"power_mw\"\n",
        "T_COL = \"air_density_kgm3\"\n",
        "X_COLS = [\n",
        "    \"wind_speed_100m\", \"wind_cubed\", \"turbulence_proxy\", \"ramp_rate\", \"wind_lag_1\",\n",
        "    \"wind_sin\", \"wind_cos\", \"hour_sin\", \"hour_cos\", \"month_sin\", \"month_cos\"\n",
        "]\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "rng = np.random.default_rng(RANDOM_SEED)\n",
        "\n",
        "# Operational-zero candidate threshold\n",
        "POWER_ZERO_THR = 0.1\n",
        "HIGH_WIND_THR = 8.0\n",
        "\n",
        "# Time-series bootstrap controls\n",
        "BOOT_B = 300\n",
        "BLOCK_LEN_HOURS = 168\n",
        "\n",
        "\n",
        "# Utilities\n",
        "def load_region_df(region, filename):\n",
        "    path = os.path.join(DATA_DIR, filename)\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(path)\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "    df = df.set_index(\"timestamp\").sort_index()\n",
        "\n",
        "    cols = [Y_COL, T_COL] + X_COLS\n",
        "    df = df.dropna(subset=cols)\n",
        "\n",
        "    # Keep physically plausible non-negative power\n",
        "    df = df[df[Y_COL] >= 0].copy()\n",
        "\n",
        "    return df\n",
        "\n",
        "def tune_xgb_regressor(X, y, n_iter=8):\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [150, 250, 350],\n",
        "        \"max_depth\": [3, 5, 7],\n",
        "        \"learning_rate\": [0.03, 0.05, 0.1],\n",
        "        \"subsample\": [0.8, 0.9, 1.0],\n",
        "        \"colsample_bytree\": [0.8, 0.9, 1.0],\n",
        "        \"min_child_weight\": [1, 3, 5],\n",
        "        \"reg_lambda\": [1.0, 2.0, 5.0]\n",
        "    }\n",
        "\n",
        "    base = XGBRegressor(\n",
        "        n_jobs=-1,\n",
        "        random_state=RANDOM_SEED,\n",
        "        tree_method=\"hist\"\n",
        "    )\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "    search = RandomizedSearchCV(\n",
        "        base,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=n_iter,\n",
        "        cv=tscv,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1,\n",
        "        verbose=0\n",
        "    )\n",
        "    search.fit(X, y)\n",
        "    return search.best_estimator_\n",
        "\n",
        "def detect_operational_zeros(df):\n",
        "    return (df[Y_COL] <= POWER_ZERO_THR) & (df[\"wind_speed_100m\"] >= HIGH_WIND_THR)\n",
        "\n",
        "def apply_scenario(df, scenario_name):\n",
        "    d = df.copy()\n",
        "\n",
        "    if scenario_name == \"A_base_filtered\":\n",
        "        d = d[(d[\"wind_speed_100m\"] > 5.0) & (d[Y_COL] > POWER_ZERO_THR)].copy()\n",
        "\n",
        "    elif scenario_name == \"B_keep_zeros_wind_gt5\":\n",
        "        d = d[(d[\"wind_speed_100m\"] > 5.0)].copy()\n",
        "\n",
        "    elif scenario_name == \"C_all_data_no_wind_filter\":\n",
        "        d = d.copy()\n",
        "\n",
        "    elif scenario_name == \"D_exclude_operational_zeros\":\n",
        "        # drop only high-wind near-zero power candidates\n",
        "        d = d[~detect_operational_zeros(d)].copy()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scenario: {scenario_name}\")\n",
        "\n",
        "    d = d.dropna(subset=[Y_COL, T_COL] + X_COLS)\n",
        "    return d\n",
        "\n",
        "def crossfit_residuals_timeseries(df, model_y, model_t, n_splits=5):\n",
        "    X = df[X_COLS].values\n",
        "    y = df[Y_COL].values\n",
        "    t = df[T_COL].values\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    ry = np.zeros(len(df))\n",
        "    rt = np.zeros(len(df))\n",
        "\n",
        "    for train_idx, test_idx in tscv.split(X):\n",
        "        my = clone(model_y)\n",
        "        mt = clone(model_t)\n",
        "\n",
        "        my.fit(X[train_idx], y[train_idx])\n",
        "        mt.fit(X[train_idx], t[train_idx])\n",
        "\n",
        "        ry[test_idx] = y[test_idx] - my.predict(X[test_idx])\n",
        "        rt[test_idx] = t[test_idx] - mt.predict(X[test_idx])\n",
        "\n",
        "    return ry, rt\n",
        "\n",
        "def residual_on_residual_effect(rt, ry):\n",
        "    # OLS slope + naive SE from scipy linregress\n",
        "    slope, intercept, rvalue, pvalue, stderr = stats.linregress(rt, ry)\n",
        "    ci_low = slope - 1.96 * stderr\n",
        "    ci_high = slope + 1.96 * stderr\n",
        "    return slope, stderr, pvalue, ci_low, ci_high\n",
        "\n",
        "def moving_block_bootstrap_slopes(rt, ry, block_len, B, rng):\n",
        "    n = len(rt)\n",
        "    if n < 5 * block_len:\n",
        "        # fallback to smaller blocks if series is short\n",
        "        block_len = max(24, int(n / 10))\n",
        "\n",
        "    n_blocks = int(np.ceil(n / block_len))\n",
        "    max_start = n - block_len\n",
        "    if max_start < 1:\n",
        "        max_start = 1\n",
        "\n",
        "    slopes = np.empty(B, dtype=float)\n",
        "\n",
        "    for b in range(B):\n",
        "        starts = rng.integers(0, max_start, size=n_blocks)\n",
        "        idx = np.concatenate([np.arange(s, s + block_len) for s in starts])[:n]\n",
        "        slope_b, _, _, _, _ = residual_on_residual_effect(rt[idx], ry[idx])\n",
        "        slopes[b] = slope_b\n",
        "\n",
        "    se_b = slopes.std(ddof=1)\n",
        "    ci_low, ci_high = np.percentile(slopes, [2.5, 97.5])\n",
        "\n",
        "    # two-sided bootstrap p-value\n",
        "    p_boot = 2.0 * min(np.mean(slopes <= 0.0), np.mean(slopes >= 0.0))\n",
        "    p_boot = float(min(1.0, p_boot))\n",
        "\n",
        "    return se_b, ci_low, ci_high, p_boot, slopes\n",
        "\n",
        "def fit_r2_diagnostics(df, model_y, model_t):\n",
        "    X = df[X_COLS]\n",
        "    y = df[Y_COL]\n",
        "    t = df[T_COL]\n",
        "\n",
        "    my = clone(model_y).fit(X, y)\n",
        "    mt = clone(model_t).fit(X, t)\n",
        "\n",
        "    r2y = r2_score(y, my.predict(X))\n",
        "    r2t = r2_score(t, mt.predict(X))\n",
        "\n",
        "    return float(r2y), float(r2t)\n",
        "\n",
        "\n",
        "# Scenarios\n",
        "SCENARIOS = [\n",
        "    \"A_base_filtered\",\n",
        "    \"B_keep_zeros_wind_gt5\",\n",
        "    \"C_all_data_no_wind_filter\",\n",
        "    \"D_exclude_operational_zeros\"\n",
        "]\n",
        "\n",
        "\n",
        "# Run Phase-06\n",
        "results_rows = []\n",
        "overlap_rows = []\n",
        "opzero_examples = []\n",
        "\n",
        "print(\"Phase-06 started\")\n",
        "\n",
        "for region, fname in FILES.items():\n",
        "    print(f\"Region: {region}\")\n",
        "\n",
        "    df0 = load_region_df(region, fname)\n",
        "\n",
        "    # Tune global nuisance learners ONCE per region (use a time-sorted sample)\n",
        "    df_tune = df0.copy()\n",
        "    if len(df_tune) > 50000:\n",
        "        df_tune = df_tune.sample(n=50000, random_state=RANDOM_SEED).sort_index()\n",
        "\n",
        "    print(\"Tuning nuisance models\")\n",
        "    model_y_global = tune_xgb_regressor(df_tune[X_COLS], df_tune[Y_COL], n_iter=8)\n",
        "    model_t_global = tune_xgb_regressor(df_tune[X_COLS], df_tune[T_COL], n_iter=8)\n",
        "\n",
        "    # Operational-zero candidate summary on FULL data (before scenario filtering)\n",
        "    oz_mask_full = detect_operational_zeros(df0)\n",
        "    oz_rate_full = float(oz_mask_full.mean())\n",
        "    oz_n_full = int(oz_mask_full.sum())\n",
        "\n",
        "    # Save some examples (top wind speeds among operational-zero candidates)\n",
        "    if oz_n_full > 0:\n",
        "        df_oz = df0.loc[oz_mask_full, [Y_COL, T_COL] + X_COLS].copy()\n",
        "        df_oz[\"region\"] = region\n",
        "        df_oz = df_oz.sort_values(\"wind_speed_100m\", ascending=False).head(200)\n",
        "        df_oz = df_oz.reset_index().rename(columns={\"index\": \"timestamp\"})\n",
        "        opzero_examples.append(df_oz)\n",
        "\n",
        "    for scen in SCENARIOS:\n",
        "        df = apply_scenario(df0, scen)\n",
        "\n",
        "        n = len(df)\n",
        "        if n < 5000:\n",
        "            # Keep consistent with earlier practice\n",
        "            continue\n",
        "\n",
        "        oz_mask = detect_operational_zeros(df)\n",
        "        oz_rate = float(oz_mask.mean())\n",
        "        oz_n = int(oz_mask.sum())\n",
        "\n",
        "        # Cross-fit residuals\n",
        "        ry, rt = crossfit_residuals_timeseries(df, model_y_global, model_t_global, n_splits=5)\n",
        "\n",
        "        # Effect + naive uncertainty\n",
        "        coef, se_naive, p_naive, ci_low_naive, ci_high_naive = residual_on_residual_effect(rt, ry)\n",
        "\n",
        "        # Time-series robust uncertainty\n",
        "        se_boot, ci_low_boot, ci_high_boot, p_boot, _ = moving_block_bootstrap_slopes(\n",
        "            rt, ry,\n",
        "            block_len=BLOCK_LEN_HOURS,\n",
        "            B=BOOT_B,\n",
        "            rng=rng\n",
        "        )\n",
        "\n",
        "        # Diagnostics\n",
        "        r2y, r2t = fit_r2_diagnostics(df, model_y_global, model_t_global)\n",
        "\n",
        "        # Overlap signals\n",
        "        rt_std = float(np.std(rt, ddof=1))\n",
        "        t_std = float(np.std(df[T_COL].values, ddof=1))\n",
        "        rt_ratio = float(rt_std / t_std) if t_std > 0 else np.nan\n",
        "\n",
        "        results_rows.append({\n",
        "            \"Region\": region,\n",
        "            \"Scenario\": scen,\n",
        "            \"N\": n,\n",
        "            \"OperationalZeroCandidates_N\": oz_n,\n",
        "            \"OperationalZeroCandidates_Rate\": oz_rate,\n",
        "            \"Effect_MW_per_kgm3\": float(coef),\n",
        "            \"Naive_SE\": float(se_naive),\n",
        "            \"Naive_p\": float(p_naive),\n",
        "            \"Naive_CI_Low\": float(ci_low_naive),\n",
        "            \"Naive_CI_High\": float(ci_high_naive),\n",
        "            \"BlockBootstrap_SE\": float(se_boot),\n",
        "            \"BlockBootstrap_p\": float(p_boot),\n",
        "            \"BlockBootstrap_CI_Low\": float(ci_low_boot),\n",
        "            \"BlockBootstrap_CI_High\": float(ci_high_boot),\n",
        "            \"R2_Outcome_inSample\": float(r2y),\n",
        "            \"R2_Treatment_inSample\": float(r2t),\n",
        "            \"Std_Treatment\": float(t_std),\n",
        "            \"Std_TreatmentResidual\": float(rt_std),\n",
        "            \"ResidualStd_to_TreatmentStd\": float(rt_ratio),\n",
        "            \"BlockLenHours\": int(BLOCK_LEN_HOURS),\n",
        "            \"BootstrapB\": int(BOOT_B),\n",
        "        })\n",
        "\n",
        "        # Overlap diagnostics by wind-speed bins (where overlap can break)\n",
        "        bins = [0, 3, 5, 7, 9, 11, 13, 50]\n",
        "        labels = [\"0-3\", \"3-5\", \"5-7\", \"7-9\", \"9-11\", \"11-13\", \"13+\"]\n",
        "        ws = df[\"wind_speed_100m\"].values\n",
        "        ws_bin = pd.cut(ws, bins=bins, labels=labels, include_lowest=True)\n",
        "\n",
        "        tmp = pd.DataFrame({\n",
        "            \"ws_bin\": ws_bin.astype(str),\n",
        "            \"t\": df[T_COL].values,\n",
        "            \"rt\": rt\n",
        "        })\n",
        "\n",
        "        grp = tmp.groupby(\"ws_bin\", dropna=False).agg(\n",
        "            N=(\"t\", \"size\"),\n",
        "            Std_T=(\"t\", \"std\"),\n",
        "            Std_RT=(\"rt\", \"std\")\n",
        "        ).reset_index()\n",
        "\n",
        "        grp[\"Region\"] = region\n",
        "        grp[\"Scenario\"] = scen\n",
        "        overlap_rows.append(grp)\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "\n",
        "# Save tables\n",
        "res_df = pd.DataFrame(results_rows)\n",
        "res_path = os.path.join(TABLE_DIR, \"Phase06_DML_Sensitivity_Bootstrap.csv\")\n",
        "res_df.to_csv(res_path, index=False)\n",
        "\n",
        "overlap_df = pd.concat(overlap_rows, ignore_index=True) if len(overlap_rows) else pd.DataFrame()\n",
        "overlap_path = os.path.join(TABLE_DIR, \"Phase06_Overlap_Diagnostics.csv\")\n",
        "overlap_df.to_csv(overlap_path, index=False)\n",
        "\n",
        "if len(opzero_examples):\n",
        "    opzero_df = pd.concat(opzero_examples, ignore_index=True)\n",
        "    opzero_path = os.path.join(TABLE_DIR, \"Phase06_OperationalZero_Candidates.csv\")\n",
        "    opzero_df.to_csv(opzero_path, index=False)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(res_path)\n",
        "print(overlap_path)\n",
        "if len(opzero_examples):\n",
        "    print(opzero_path)\n",
        "\n",
        "# Operational-zero candidates scatter (one panel per region)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
        "\n",
        "for i, (region, fname) in enumerate(FILES.items()):\n",
        "    df0 = load_region_df(region, fname)\n",
        "    c = ISLAND_COLORS[region]\n",
        "    oz = detect_operational_zeros(df0)\n",
        "\n",
        "\n",
        "    df_plot = df0[[Y_COL, \"wind_speed_100m\"]].copy()\n",
        "    if len(df_plot) > 120000:\n",
        "        df_plot = df_plot.sample(n=120000, random_state=RANDOM_SEED).sort_index()\n",
        "        oz = oz.loc[df_plot.index]\n",
        "\n",
        "    axes[i].scatter(df_plot[\"wind_speed_100m\"], df_plot[Y_COL], s=6, alpha=0.12, color=\"gray\", linewidths=0)\n",
        "    if oz.any():\n",
        "        axes[i].scatter(df_plot.loc[oz, \"wind_speed_100m\"], df_plot.loc[oz, Y_COL], s=8, alpha=0.6, color=c, linewidths=0)\n",
        "\n",
        "    axes[i].set_title(f\"Operational-zero candidates ({region})\")\n",
        "    axes[i].set_xlabel(\"wind_speed_100m (m/s)\")\n",
        "    if i == 0:\n",
        "        axes[i].set_ylabel(\"power_mw (MW)\")\n",
        "    axes[i].grid(alpha=0.2)\n",
        "\n",
        "fig.suptitle(f\"Operational-zero candidates: power_mw <= {POWER_ZERO_THR} with wind_speed_100m >= {HIGH_WIND_THR}\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(PLOT_DIR, \"Phase06_OperationalZeros_Scatter.png\")\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "plt.close()\n",
        "\n",
        "# Operational-zero rate by scenario (bar chart)\n",
        "if not res_df.empty:\n",
        "    plot_df = res_df.copy()\n",
        "    plot_df[\"ScenarioLabel\"] = plot_df[\"Scenario\"].map({\n",
        "        \"A_base_filtered\": \"Base filtered\",\n",
        "        \"B_keep_zeros_wind_gt5\": \"Keep zeros (wind>5)\",\n",
        "        \"C_all_data_no_wind_filter\": \"All data\",\n",
        "        \"D_exclude_operational_zeros\": \"Exclude op-zeros\"\n",
        "    })\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 5))\n",
        "\n",
        "    # Arrange grouped bars\n",
        "    scen_order = [\"Base filtered\", \"Keep zeros (wind>5)\", \"All data\", \"Exclude op-zeros\"]\n",
        "    regions = [\"North Island\", \"South Island\"]\n",
        "    x = np.arange(len(scen_order))\n",
        "    w = 0.35\n",
        "\n",
        "    for j, region in enumerate(regions):\n",
        "        sub = plot_df[plot_df[\"Region\"] == region].set_index(\"ScenarioLabel\").reindex(scen_order)\n",
        "        y = sub[\"OperationalZeroCandidates_Rate\"].values\n",
        "        ax.bar(x + (j - 0.5) * w, y, width=w, color=ISLAND_COLORS[region], alpha=0.85, label=region)\n",
        "\n",
        "    ax.set_title(\"Operational-zero candidate rate by scenario\")\n",
        "    ax.set_xlabel(\"Scenario\")\n",
        "    ax.set_ylabel(\"Rate (fraction of hours)\")\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(scen_order, rotation=0)\n",
        "    ax.grid(axis=\"y\", alpha=0.2)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(PLOT_DIR, \"Phase06_OperationalZero_Rate.png\")\n",
        "    plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
        "    plt.close()\n",
        "\n",
        "print(\"Plots saved:\")\n",
        "print(os.path.join(PLOT_DIR, \"Phase06_OperationalZeros_Scatter.png\"))\n",
        "print(os.path.join(PLOT_DIR, \"Phase06_OperationalZero_Rate.png\"))\n",
        "\n",
        "print(\"Phase-06 complete\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}